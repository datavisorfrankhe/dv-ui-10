# Rules Engine

## Introduction

### Definitions

A **Rule** is the fundamental component of **Rules Engine**:
  - A **Rule** consists of an "IF (**Condition**)" and "THEN (**Action**)" logic: IF the conditions are met, THEN the actions will be carried out.

In an “IF (Condition),” one or multiple conditions can be concatenated with “AND/OR/NOT” logic.

***Example:***
Concatenate **Condition 1**: “User_name” contains “John” and **Condition 2**: “Checkout_time” < 5 seconds
- Concatenated: (“User_name” contains “John” AND “Checkout_time” < 5 seconds)

As of now, DataVisor supports two types of rules: **Manual Rules** and **Automated Rules**. As the name suggests, **Manual Rules** are created by analysts (Rules Engine users) through the **Create Rule** page. **Automated Rules** are generated by DataVisor’s Machine Learning models  and are updated frequently in your Rules Engine based on the dataset we received: this is an add-on functionality with additional purchase options.

When an event or user is detected by a rule in Rules Engine, DataVisor sends a response in a pre-approved format to the client’s Backend. In the response, we include the **Action** and **Action Data** that you indicated when the rule was created.

A **Rule Set** is a group of rules which can be a combination of Manual Rules and Automated Rules. The diagram below is an illustration of how the elements of the Rules Engine are tied together:

> Figure 1: Rule Set Diagram
![image-1]

### Workflow

> Figure 2: Rules Engine Workflow
![image-2]

To use Rules Engine for your daily operations, you can **Create Rule**, **Test Rule**, and **Manage Rule**.

**Create Rule:**
- **Add Conditions ("IF" Logic):** When an entity (user or event) is sent to Rules Engine, the entity will be checked against the conditions set in a rule. If these conditions are met, the actions in “THEN” logic will be triggered.
  - ***Example:*** IF ("Transaction amount" > 5000 AND "Checkout speed" < 10)
- **Specify Actions (“THEN” logic)**: Once conditions in “IF” are met, all selected actions will be triggered.
  - ***Example:*** THEN block (this transaction) AND send Data Action = “High velocity transaction"
- **Additional Details:** Additional information such as Tags, Scores, etc... can be provided for each rule in order to assist rules management. This data can be included in DataVisor’s response to your company. Please discuss with your Engineering team and DataVisor’s Technical Account Manager for more details.

**Test Rule:**
- **Basic Test:** Test one rule on a specific dataset. This is a quick test and it will not return comprehensive results against the other rules.
- **Comparison:** Compare two rules or two rule sets on a selected dataset or data range. The test will return which entities (e.g., users or events) are overlapped (i.e., matching multiple rules) or uniquely matching one or the rules.
- **Simulation:** Test the potential impact of adding and/or removing a number of rules to an existing published set of rules. By default, all published rules are included in the simulation. This test generally takes more time to complete but it produces more comprehensive results about unique and overlapped entity detection.

**Manage Rules:**
- **Rule Analytics:** Analytics tool of Rules Engine which can be found in the **Rule Details** page (more details described below) where rule conditions and the performance of the rule over time can be viewed.
- **Edit/Delete Rule:** A rule can be edited or deleted in “Draft” mode. For more details, see **Edit/Delete Rule** below.
- **Add Rule Set:** Multiple rules can be organized into a rule set for better management.

## Create Rule

### Define Conditions

A condition has three components: **Feature**, **Operator**, and **Value** in which the syntax is expressed as the following:

| Feature  | Operator | Value |
| :--------: |:--------:|:-----:|
| email_verify | STARTS_WITH | john |

- **Features:** Derived from raw fields or generated by Feature Platform. Multiple features can be concatenated with AND/OR/NOT logic.
- **Operators:** How a feature can be processed, see **Section 2.1.2: Operators** for more details.
  - **Note:** Operator is dependent on the user-defined data type
- **Values:** Specified sequence of elements.

#### Features

You can select a feature from the drop-down list which includes raw features from your data as well as derived features generated by DataVisor’s Feature Platform. If you have purchased Feature Platform, you can create a new feature in Feature Platform and it will appear in **Create Rule** for you to use.

You can also click **Explore Features** to view all features available as illustrated below:

> Figure 3: Explore Features
![image-3]

A pop-up will be displayed in which you can view the details of each feature, including **Type**, **Description**, and **Creator**, as illustrated in the figure below:

> Figure 4: Feature Details
![image-4]

**Data Type:** Below are the data types that DataVisor can process.
- **Boolean:** takes the value of “TRUE” or “FALSE”
- **Numeric:**
  - **Integer:** Represents positive, negative, or zero whole number (32 bits) that is not a fraction.
    - ***Example:*** 1 or 100
  - **Double:** Represents double precision floating point data-type (64 bits) that can represent both fractions and whole numbers including 0.
    - ***Example:*** 1.32, 323.2, 120.00, 120
  - **Long:** Represents numbers (up to 64 bits) that are too large to be represented as an *int* or *Integer* including 0.
    - ***Example:*** 9,223,372,036,854,775,807
- **String:**
  - **Text:** Represents text rather than numbers. Consists of a set of characters that can also contain spaces and numbers.
  - **Normalized:** Fraudsters often try to use Unicode superscript or subscript to bypass any rules that monitor for suspicious strings, such as “Please text<sup>me</sup> directly to discuss<sub>special prices.</sub>” DataVisor will normalize these special strings to “Please text me directly to discuss special prices,” so that your rule will detect such text.

#### Operators

Operators will be shown based on the selected feature’s data type. Similarly to **Explore Features**, you can also select **Explore Operators**.

**Note:** A feature must first be selected in order to select an operator or view **Explore Operators**.

> Figure 5: Explore Operators
![image-5]

Below is the list of common string operators with examples on how to use each operator:

- **STR_EQ**, **STR_EQ_NORMALIZED:** Checks if the feature matches (==) string value exactly.
  - ***Example:***
    | Feature  | Operator | Value |
    | :--------: |:--------:|:-----:|
    | user_id | STR_EQ | fraud.user@gmail.com |
  - **Note:** If the raw data is fraud.user@<sub>gmail.com</sub>, you should use operator STR_EQ_NORMALIZED to detect this user. The operator will first transform the special string to a normal string (“fraud.user@gmail.com”) and then check the exact match value that you have in rule.

    > Figure 6: STR_EQ Operator Example
    ![image-6]

- **STR_NE**, **STR_NE_NORMALIZED:** Checks if the feature does not exactly match (!=) with string value. (Same as above example except with a “not” condition)
  - ***Example:***
    | Feature  | Operator | Value |
    | :--------: |:--------:|:-----:|
    | email | STR_NE | fraud.user@gmail.com |
    | email | STR_NE_NORMALIZED | fraud.user@<sub>gmail.com</sub> |
  - **Note:** STR_NE_NORMALIZED operator will normalize the string’s subscript or superscript first.
 - **STR_CONTAINS**, **STR_CONTAINS_NORMALIZED:** Checks for the presence of a specified sequence of substring. This can be a powerful tool for detection. For example, a fraudulent user may have multiple IPs; If you know 17.142.180.22 is a suspicious IP address, you may be able to detect several other suspicious IP addresses.
   - ***Example:***
     | Feature  | Operator | Value |
       | :--------: |:--------:|:-----:|
       | user_ip | STR_CONTAINS | 17.142.180.22 |
       | message | STR_CONTAINS_NORMALIZED | <sub>contact me</sub> |
   - **Note:** STR_CONTAINS_NORMALIZED operator will normalize the string’s subscript or superscript first.
- **STR_STARTS_WITH**, **STR_STARTS_WITH_NORMALIZED:** Check if the string starts with a specified sequence of elements.
  - ***Example:***
    | Feature  | Operator | Value |
    | :--------: |:--------:|:-----:|
    | user_id | STR_STARTS_WITH | fraud@ |
    | user_id | STR_STARTS_WITH_NORMALIZED | fraud |
  - **Note:** STR_STARTS_WITH_NORMALIZED operator will normalize the string’s subscript or superscript first.
- **STR_ENDS_WITH**, **STR_ENDS_WITH_NORMALIZED:** Check if the array ends with a specified sequence of elements.
  - ***Example:***
    | Feature  | Operator | Value |
    | :--------: |:--------:|:-----:|
    | user_id | STR_ENDS_WITH | @gmail.com |
    | user_id | STR_ENDS_WITH_NORMALIZED | <sub>@gmail.com</sub> |
  - **Note:** STR_ENDS_WITH_NORMALIZED operator will normalize the string’s subscript or superscript first.
- **STR_REGEX**, **STR_REGEX_NORMALIZED:** Regex refers to regular expression, a method for string and pattern matching.
  - Refer to the section below for more details and examples.
- **STR_IN_LIST:** Check if the string value is contained in a specified list.
  - To input a list, use “/n” (or press Enter) as a separator between objects in the list.

    > Figure 7: Input List for Value
    ![image-7]

  - For adding to long lists, type the list in any text editor software and copy and paste it over (as long as “/n” is the separator).
  - Once the rule is created, verify the list in **Rule Condition** as [value1, value2, … value(n)].

    > Figure 8: Rule Condition
    ![image-8]

Below is the list of common operators for numeric features with examples on how to use each operator.

- **DOUBLE_EQ**, **INT_EQ**, **LONG_EQ:** EQUAL TO (==) operator for double, int, and long, respectively (defined above).
- **DOUBLE_NE**, **INT_NE**, **LONG_NE:**
NOT EQUAL TO (!=) operator for double, int, and long, respectively.
- **DOUBLE_GT**, **INT_GT**, **LONG_GT:** GREATER THAN (>) operator for double, int, and long, respectively.
- **DOUBLE_GE**, **INT_GE**, **LONG_GE:**
GREATER THAN OR EQUAL TO (>=) operator for double, int, and long, respectively.
- **DOUBLE_LT**, **INT_LT**, **LONG_LT:**
LESS THAN (<) operator for double, int, and long, respectively.
- **DOUBLE_LE**, **INT_LE**, **LONG_LE:**
LESS THAN OR EQUAL TO (<=) operator for double, int, and long, respectively.

#### Values

After selecting the feature and operator, a box to **Enter Value** will be displayed.

For List operators, or operators that take a list as input value, you can **Upload a file** at your convenience. Examples of files to upload include a blacklist or whitelist file. This functionality only appears when the List operator is selected. We recommend a *.txt* file in which you input each value in one row.

***Example:***
Value 1
Value 2

**Note:** To separate each value in the **Enter Value** box, you can use row separation, or “/n”.

> Figure 9: Upload List File for Value
![image-9]

#### Copy & Delete Condition

After adding a complete condition, you can conveniently **Copy** the condition by clicking the copy icon as illustrated below.

> Figure 10: Copy Condition
![image-10]

Next to the **Copy** icon, you can click the trash can icon to delete the condition as well.
**Note:** Once a condition has been deleted, it cannot be reversed.

#### Create a Regular Expression (REGEX)

References & more details can be found in articles like [this](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285) on how you can use REGEX.

DataVisor supports REGEX as an input value. You can take advantage of this functionality as described below.

| Metacharacter | Definition |
| :-------- |:--------|
| ^ | Start of a string. |
| $ | End of a string. |
| . | Any character (except \n newline). |
| \| | Alternation. |
| {...} | Explicit quantifier notation. |
| [...] | Explicit set of characters to match. |
| (...) | Logical grouping of part of an expression. |
| * | 0 or more of the previous expressions. |
| + | 1 or more of the previous expressions. |
| ? | 0 or 1 of previous expression; also forces minimal matching when an expression might match several strings within a search string. |
| \ | When preceding another metacharacter, that metacharacter becomes a literal instead of a special character. When preceding a special matching character, see below. |

| Character Class | Definition |
| :---------------- | :------ |
| . | Matches any character except “\n”. If modified by the “Singleline” option, a period character matches any character. For more information, see [Regular Expression](https://regexone.com/) for more details. |
| [aeiou] | Matches any single character included in the specified set of characters. |
| \^[aeiou] | Matches any single character not in the specified set of characters. |
| [0-9a-fA-F] | Use of a hyphen (“–”) allows for specification of contiguous character ranges. |
| \p{name} | Matches any character in the named character class specified by {name}. Supported names are Unicode groups and block ranges. For example, Ll, Nd, Z, IsGreek, IsBoxDrawing. |
| \P{name} | Matches text not included in groups and block ranges specified in {name}. |
| \w | Matches any word character. Equivalent to the Unicode character categories [\p{Ll}\p{Lu}\p{Lt}\p{Lo}\p{Nd}\p{Pc}]. If ECMAScript-compliant behavior is specified with the ECMAScript option, \w is equivalent to [a-zA-Z_0-9]. |
| \W | Matches any nonword character. Equivalent to the Unicode categories [^\p{Ll}\p{Lu}\p{Lt}\p{Lo}\p{Nd}\p{Pc}]. If ECMAScript-compliant behavior is specified with the ECMAScript option, \W is equivalent to [^a-zA-Z_0-9]. |
| \s | Matches any white-space character. Equivalent to the Unicode character categories [\f\n\r\t\v\x85\p{Z}]. If ECMAScript-compliant behavior is specified with the ECMAScript option, \s is equivalent to [ \f\n\r\t\v]. |
| \S | Matches any non-white-space character. Equivalent to the Unicode character categories [^\f\n\r\t\v\x85\p{Z}]. If ECMAScript-compliant behavior is specified with the ECMAScript option, \S is equivalent to [^ \f\n\r\t\v]. |
| \d |
Matches any decimal digit. Equivalent to \p{Nd} for Unicode and [0-9] for non-Unicode, ECMAScript behavior. |
| \D | Matches any nondigit. Equivalent to \P{Nd} for Unicode and [^0-9] for non-Unicode, ECMAScript behavior. |

### Define Action

#### DataVisor Actions

DataVisor provides **Customized Actions** capability depending on your needs. If you need **Customized Actions**, please contact our customer support.

By default, the connecting logic between actions is “AND,” meaning that if you select two actions, both actions will be executed if the conditions (IF) are met. DataVisor actions are system-defaults and hence unable to be deleted. To add a new action, please contact your Technical Account Manager for assistance.

For more information about how the action works with your internal system, please refer to **Section 2.2.2: Action & Response**.

DataVisor Actions include but are not limited to:

| Action | Definition |
| :---------------- | :------ |
| **ACCEPT** | Allow an event to executive or approve an user, to be defined at the client’s discretion. |
| **REJECT** | Prevent an action or behavior in response to a rule. |
| **LABEL** | Send a label with customizable **Action** data. After selecting this action, you will be able to enter text or select a label in **Action** data input. When an event or user is detected by this rule, we will label that entity with the **Action** data that you indicate. |
| **REVIEW** | Send the result of a rule to the Operation Team for further investigation. |
| **SCORE** | Allow Rules Engine to include the score of the rule in the response to your company’s Backend. For example, if a user is detected by this rule and the input for "Score" is 0.85, we can include "score: 0.85" in the response. |
| **WHITELIST** |  Classify as “Good”; users or events. |
| **BLACKLIST** | Classify as “Bad”; users or events. |
| **RUN RULE SET** | If this rule’s conditions are met, we will then run the selected rule set. ***Example:*** 1. In Rule A, we set an action as “Run Rule Set” & select “Rule Set B”. 2. When a user X meets the conditions set in Rule A, it will then also run “Rule Set B” to check against user X. |
| **SET VALUE** | When this action is selected, you can select a score feature to set its value. In the drop down menu, we display all features available. ***Example:*** 1. You want to create a rule that if the conditions are met, you select this action and the rule will add 10 to the score feature. If the score reaches a certain threshold, you can create another rule to make a decision. **Rule 1:** If value > $10,000, then you can **SET VALUE** to add 10 to the score. **Rule 2:** Score > 90, then you can **REVIEW**. 2. Depending on the type of feature selected, you can then select one of two operators: **Add** or **Subtract**. 3. Finally, you will enter a value. DataVisor will also check if the entered value’s data type matches with the feature's data type. For example, for **Risk Score**, the values must be numeric.

**Count Functionality:**

The **Count** functionality provides a quick way for the Operation Team to set a rule to monitor the efficacy of rules and user behavior when a specified number of times the rules is matched or triggered

> Figure 11: Count Functionality
![image-11]

***Example 1:***
You want to reject a user who violates the policy of not mentioning their personal phone number in their product’s description 3 times.

**Rule 1:** If “product_desc” **CONTAINS** “phone number”, then count the number of times that this rule is triggered. You can name this counter “count_phone_mentioned”.

**Rule 2:** If “count_phone_mentioned” > 3, then set the **Action** to **REJECT**.

For this example, check the box for “Count the number of times that this rule is triggered per user.” Enter the name for this counter; you will then be able to select this as a feature in Conditions (IF). Enter a description to remind others what the counter is for.

***Example 2:***
**Create Rule:** "user_ip_country" NOT EQUAL "Germany"
**Count:** Number of IP count outside of Germany for each user

#### Action & Response

When an event or user is detected by a rule in Rules Engine, DataVisor sends a response in a pre-approved format to the client’s Backend. In the response, we include the **Action** and **Action Data** that you indicated when the rule was created.

***Example:***
Create a rule that detects any transaction larger than $5,000, set the **Action** as **LABEL** for this user, and set **Action Data** as “Very high value transaction.” The response might look like this:

> Figure 12: Action Response Example
![image-12]

Although DataVisor provides the Action and Action Data for your team, you ultimately have full authority and responsibility over how the response is processed on your end accordingly.

### Organize Rules Into Rule Set

Located at the top right corner of Rules Engine’s **Home** page, you can create a **Rule Set** by selecting the rules to include in the new Rule Set.

> Figure 13: Create Rule Set
![image-13]

For your convenience, we provide filters in the rule selection table. For example, you can filter to look at only **MANUAL RULES** or **AUTOMATED RULES** as you select which rules to include.

In the **Create Rule** page, you can quickly test your rule by clicking on **Test Rule**: the rule will be automatically saved in “Draft” mode and you will be directed to the **Test Center** to select the rule for testing. Please refer to **Section 3.1: Basic Test** for more details.

## Test Rule

Our testing capabilities are built around the **Test Center**, which is accessed through the **Home** page of Rules Engine. In the **Test Center**, you can perform different types of testing: **Basic Test**,  **Comparison**, or **Simulation**.

### Test Center

#### Use Cases

Test Center can be used for different purposes:
- **Basic Test:** Designed to test individual rules or rule sets. This test can be used to ensure that a newly created rule will detect a suspicious event or user that you intend to catch or to verify if an existing rule is working as intended.
- **Comparison:** Designed to compare two sets of rules so that you can decide whether to retire one of the sets.
- **Simulation:** Allows you to test if adding some rules to the production will increase the number to detected users/events. For example, you might have 200 rules running on production, and based on the newly emerged patterns, you create 3 new rules. You now can test if 203 rules can detect more than what 200 existing rules can.

**Note:** **Comparison** and **Simulation** are advanced options that are available as add-ons. A separate purchase is required.

#### Access Test Center

**Test Center** is positioned on the top right corner of **Home** page. You can also access the **Test Center** while creating a rule; when clicking on **Test Rule** under **Create Rule**, the rule will temporarily be created in “Draft” mode and you will be directed to the window illustrated below.

> Figure 14: Access Test Center
![image-14]

For your convenience, the latest job’s result will be kept until a new job is run.

### Basic Test

**Basic Test** can be accessed in the **Test Center** page as illustrated below.

> Figure 15: Basic Test
![image-15]

#### Test a Rule vs Rule Set

We provide additional functionality to test a rule set. You can select whether to test a rule or a rule set by toggling the **Rule** or **Rule Set** button as shown in the screenshot below.

> Figure 16: Toggle Rules or Rule Set
![image-16]

If **RULE SET** is selected, the test returns the result for all rules within the rule set. An additional table will be provided to summarize the results of each rule for total detections.

#### Select Time Range

When selecting the time range, you can choose from different options: **Custom**, **Last 1 day**, **Last 3 day**, and **Last 7 day**. The “Last X day” options are calculated by hours. For example, 7 days means 168 hours since the last detection time that we received from your Backend system.

**Note:** The “Last X day” options are dynamic moving time range selections, meaning that they are calculated based on the latest data point that your system sends to DataVisor.

***Example:***
If the latest data point event time is “April 28, 13:50”, and you select **Last 1 day**, DataVisor will populate the data from “04/27/2020 13:50” to “04/28/2020 13:50”.

#### Test Against Manual Input

This is an advanced option that allows you to check if the syntax (condition logic) of a rule or rule set is correct. In this test, you can manually provide a row of data (e.g., a user or an event) in JSON format to test against a selected rule or rule set.

> Figure 17: Test Against Manual Input
![image-17]

- We treat the inputs in JSON format as “attribute-value” pairs. The Rules Engine will check against the logic of the selected rules.
- If an attribute is mentioned as a feature in the rule condition but is not present in the manual input, we will treat it as if that condition is not met.

#### Result Table for Basic Test

An example of the **Result Table** is shown below.

> Figure 18: Basic Test Result Table
![image-18]

**Detection Attributes** are the features or attributes that were triggered in the rule conditions. For example, if Rule A has the conditions (IF "Transaction" > $4,000 OR "Device Agent" = “android 4.0”), then the detection attributes here are “Transaction” and “Device Agent”.

Triggered values of detection attributes will be shown and highlighted. In the example above, the value “$5,000” will be highlighted because it is the value of the “Transaction” detection attribute that triggered Rule A.

In the **Result Table**, we also provide functionalities such as **Export**, **Search**, and **Sort**. If you need additional customized functions on the table, please contact your Technical Account Manager.

**Note:** Adding additional functionalities may result in a degradation of the table.

#### Test Against User ID or Event ID

This test allows you to check if the syntax of a rule or rule set is correct by checking it against specific users or specific events. You can select whether the checking will be performed at the user level or at the event level.

In order to test for multiple users or multiple events at one time, you can use a comma (“,”) as the separator.

> Figure 19: Test Multiple Users (or Events)
![image-19]

The **Result Table** is displayed in the same manner, with the number of detected users (or events) on top and which users (or events) among those were detected by this rule or rule set.

### Comparison Test

**Comparison** is an advanced functionality that may require an additional purchase to enable.

This test helps measure the impact of replacing a rule (or rule set) with another rule (or rule set). This test allows for faster results if the change is minor. You can select a historic data range. For your convenience, we provide 3 default options: **Last 1 day**, **Last 3 day**, and **Last 7 day**.

#### Select Rule or Rule Set

Just as in **Basic Test**, you can toggle between **Rule** and **Rule Set** to select either a rule or rule set to be tested, as illustrated below.

> Figure 20: Comparison
![image-20]

#### Result Table for Comparison

An example of the **Result Table** for **Comparison** is shown below.

> Figure 21: Comparison Result Table
![image-21]

The net impact of Rule A is the number of **Unique Detections** found only by Rule A. Similarly, the net impact of Rule B is the number of **Unique Detections** found only by Rule B.

Meta analytics provide an overview of detection results for Rule A and Rule B, including **Total Detection**, **Unique Detection**, and **Overlapped Detection**. For definitions of those types of detections, please refer to **Section 4.1.1: Rule Metadata**.

In the **Result Table**, we show all detection results. However, you can choose to filter the detection results by those detected only d by Rule A, or only by Rule B, or by both Rule A and Rule B.

### Simulation Test

You can select **Simulation** from the **Test Center** page.

> Figure 22: Simulation
![image-22]

#### Select Rules for Simulation

By default, we include all published rules at the time of the test. However, you can select which published rules you want to remove and which rules in “Trial” or “Draft” mode you want to put in the **Simulation**.

When you select rules to include in the **Simulation**, the rules will be highlighted, and the count for how many rules are currently selected will also be updated in real time, as shown below.

> Figure 23: Select Rules for Simulation
![image-23]

#### Result Table for Simulation

An example of the **Result Table** is shown below.

> Figure 24: Simulation Result Table
![image-24]

The net impact of the simulated rules can be calculated by the following formula: “Unique Detections by Simulation Rules” - “Unique Detections by Existing Published Rules”. As a result, this net impact can be negative if the simulation rules detect fewer unique results.

The **Result Table** provides an overview of detection results (**Total Detections**, **Unique Detections**, and **Overlapped Detection**) of the existing rules and the simulation rules during the same time frame. By displaying the differences between the existing production rules and the simulation rules, you can study the impact of the simulation rules.

## Manage Results

### Rules Analytics

DataVisor provides analytics data of each rule in its **Rule Details** page.

To access the **Rule Details** page, click on any rule name which is displayed in a hyperlink format from the **Rules** table on the **Home** page.

> Figure 25: Access Rule Details
![image-25]

You will be directed to the **Rule Details** page of that particular rule as shown below.

> Figure 26: Rule Details
![image-26]

#### Rule Metadata

We have 3 different types of detections: **Total Detections**, **Unique Detections**, and **Overlapped Detections**. A rule’s **Unique Detections** is the number of results (e.g., users or events) detected solely by that rule. A rule’s **Overlapped Detections** is the number of results detected by that rule as well as one or more other rules. The example below explains these concepts.

***Example:***

**Rule 1:** Detected users {A, B, C, D, E}

**Rule 2:** Detected users {A, E}

Here, the **Unique Detections** count for Rule 1 is 3 because 3 users {B, C, D} were detected by only Rule 1 but not Rule 2. On the other hand, the **Unique Detections** count for Rule 2 is 0 because Rule 1 also detected users {A, E} that Rule 2 detected.

Conversely, the **Overlapping Detections** count for Rule 1 is 2 because 2 users {A, E} that were detected by Rule 1 were also detected by Rule 2. Similarly, the **Overlapping Detections** for Rule 2 is 2 because the 2 users {A, E} that were detected by Rule 2 were also detected by Rule 1.

The **Total Detections** count for Rule 1 is 5 because Rule 1 detected 5 users {A, B, C, D, E}. The **Total Detections** count for Rule 2 is 2 users because Rule 2 detected 2 users {A, E}.

Beside these 3 different types of detection, you can also view other information in the **Rules Details** page such as:
- **Rule ID**, **Rule Score**, **Created by**, **Created time**
- **Last updated time**, **Last updated by** which displays the what time the rule was most recently updated and which team member has updated that rule.
- If you have enabled the **Queue Management** function (a separate product offering), the queue that the rule is associated with will also be displayed.

#### Rule Conditions

By default, the **Performance Chart** will display information from the last 7 days of detection. However, you can adjust the display by selecting a different time range of your choice. When you select a new time range, the **Performance Chart** and the **Result Table** will both be updated.

The **Result Table** is filtered based on the time range selected (located above the **Performance Chart**). Please check **Section 3.1.5: Detection Result Table** for more information.

#### Overlap Analysis

**Overlap Analysis** is a visual graph displaying which other rules have the most overlapping detection results with the current selected rule. In this graph, DataVisor displays only the top 2 rules that share the most overlapping detection results with the current rule.

The **Overlap Analysis** graph displays data based on two time range options: the latest date (today) or the last 7 days.

**Note:** Depending on the selected time range, the top 2 rules with the most overlapping detection results with the current rule may be different.

The **Result Table** below the **Overlap Analysis** chart is also dynamically shown based on which of the two time ranges is chosen. Because the datasets from the latest date and the last 7 days are different, their respective detection results may differ.

### Toggle User or Event

This functionality is provided if you have a mixed set of rules both at the user level and the event level. It will determine if the **Result Table** in each **Rules Details** page as well as the various **Result Tables** in the **Test Center** page will be shown in terms of detected users or detected events.

> Figure 27: Toggle Event or User
![image-27]

***Example:***
If you toggle to select **Event**, when clicking on a rule to visit its **Rule Details** page, you will find each detection row on **Rule Performance** corresponds to an event that begins with an Event ID. Similarly, if you toggle to select **User**, each detection row on **Rule Performance** will correspond to a user beginning with a User ID.

# dCube

## Introduction

### Executive Summary
DataVisor, the renowned and leading company in fraud detection and prevention, extends its fraud solutions with **dCube**, a platform for customizable modeling. This document is intended to guide the process workflow of **Manual Modeling** on the **dCube** platform while explaining the algorithmic logic behind the model.

### Unsupervised Machine Learning Algorithm Overview
DataVisor's patented and proprietary **Unsupervised Machine Learning (UML)** algorithm is the highlight of dCube modeling.

Common trending algorithms for fraud detection include clustering techniques to explore relationships and connectivity among users from input data as well as anomaly detection techniques to identify and mitigate outliers. dCube’s UML transcends common clustering and graphical techniques by bridging advanced clustering and graphical analysis techniques with outlier filtering post configuration. Unlike techniques like k-means clustering, DataVisor’s proprietary UML algorithm can scale linearly with respect to the number of data points and features, making it ideal for detecting large coordinated patterns. UML assesses cluster patterns, trends, and hyper-specific characteristics while minimizing outliers with granularity and scalability.

The following section explains the UML conceptually.

#### Data Organization
Before running the UML algorithm, the system organizes your raw data in terms of user-based entities and various events with valid timestamps. The data is arranged so that users can ultimately be classified as fraudulent or not.

***Example:***
| User ID  | cookie | card | Application date | ip |
| :--------: |:--------:|:-----:|:-----:|:-----:|
| user01 | cookie01 | card05 | 01022019 | 1.1.3.4 |
| user01 | cookie02 | card04 | 01012019 | 1.1.3.5 |

#### Feature Selection and Subspace
Once the data is organized at the user level, feature subspaces are formed with different features as dimensions, and users are plotted in the subspace based on their feature values.

Although the system automatically creates feature subspaces, you can control which subspaces are formed and mandate the system to include specific features through feature selection.

Features can be selected and additional configurations can be added to set either individual features (High Priority) or combination features (Combine Features) as must-have dimensions. This will be the seed subspace, and other features for the subspace are automatically determined to make it higher dimensional. If a high-priority configuration is set, the subset should have at least one dimension from the must-have (High Priority) list of features when it is formed.

***Example:***
Feature selection: Select 10 features such as F1, F2, F3, F4...F10.

High Priority features: Select {F2} as high priority feature

Combine Features: {F6, F7} are combined and set as a high priority feature by default.

In the background, every subspace should have {F2} OR {F6, F7} as a dimension. In this case permutations of feature dimensions are used for creating subspaces. For example, feature subspaces such as subspace1 {F1, F2, F3}, subspace2 {F2, F3, F4}, subspace3 {F1, F2, F4, F5}, subspace4 {F1, F6, F7}, etc...will each have at least one of the high priority features. High priority features can be individual features or a combination features. In this specific case, a subspace{F1, F3, F4} will not be created since neither of the high priority features exists. Additionally, feature subspace {F1, F6, F8} will not be created since F6 is only high priority in combination with F7 and not a high priority feature by itself.

**What Makes a Good High Priority/Combination Features?**
Entities (categorical features) that can identify where a pattern comes from are good candidates (e.g. IP, device, and receiver accounts for fraud use cases). Sometimes, a concatenation of several entities can better capture a pattern (e.g. IP range together with timestamp), making it a combination of features.

#### Clustering
**Pairwise Linkage Clustering Technique**
With linkage function defined, clustering is performed in a feature subspace and clusters are identified. After all users have been plotted in the subspaces, users are clustered based on how close they are in a given space based on feature values. User entities are clustered with a proprietary algorithm derived from the pairwise linkage function. Pairwise linkage functions reveal the interconnectivity of users within a cluster. The linkage between two users in a feature subspace is determined by the set of features they share and their shared values. In general, more shared features result in a higher linkage value. For a specific feature, if users in a subspace share a value that is globally rare, then the linkage value from that specific feature will be higher.

You have the configuration to include or exclude specific values within a feature to determine the linkages. This is especially impactful in pairwise linkage function as the distribution of feature values will be severely impacted after including excluding specific default values.

**Filter Clusters**
After the clusters are formed based on pairwise linkage, you have the option of filtering clusters based on size (minimum number of users per cluster).

Additionally, outlier filtering is supported at the user level and at the cluster level. Users within a cluster are filtered if they are very different from the rest of the users.
> Figure 1: Filter Clusters
![dcube-image-1]

**NOTE:** Filtered clusters are not part of the final model output.

**Score Clusters**
The Cluster Suspicious Index (CSI) is a scoring method to assess a cluster’s risk factor on a scale from 0-1. A score closer to 1 indicates high suspicion. For each cluster, its score is a weighted sum over all feature contributions scaled by cluster size. You have the ability to configure the share threshold (X%) for each feature. A feature is counted in the final score only if more than X% of users share a value for this feature in the cluster. Oftentimes, a feature may have several default values which should be avoided in user linkage or cluster score. Such scenarios can be avoided by using the option to include or exclude keyword values for each feature.

Sometimes, there may be correlated features in a subspace; you can discount or ignore feature contributions from a correlated feature by specifying the feature family. DataVisor’s auto UML has a built-in algorithm that detects and handles correlated features. Default values are shown for correlation as pre-configurations, and you have the option to edit or add based on your data. Within a feature family, parent correlation has a higher feature importance during scoring.  

The correlation configuration also optimizes the feature subspace. A cluster with a score of 0 will be automatically filtered.

**Numerical Features in UML**
Since we are determining how similar users are, categorical features makes it easy to compare values. In order to better utilize a numerical feature in modeling, you can preprocess it into a categorical feature by bucketing the feature into different sets based on business logic or risk factors. For example, the numerical feature can be bucketed into different ranges such as 0-1000, 1000-10000, etc...to compare all users within a specific range instead of a specific value.

## Data Management
### Import Data
The first step of building a model using dCube is to import your data into your dCube. dCube supports common file formats from various data sources and offers a data validation tool to easily check your data quality before building the model.
> Figure 2: Import Data
![dcube-image-2]

### Navigation
There are two ways to access the file import page.
1. Navigate to the **Home** page and click on the **Get Started** button.
   >Figure 3: Import Data Step 1
   ![dcube-image-3]
2. Navigate to the **Home** page and click on the **Manage Data** button in the **Data Management** box. On the top left of the **Data Management** tab, click on the **Import Data** button.
   >Figure 4: Import Data Step 2
   ![dcube-image-4]

#### Supported File Formats
dCube currently supports files in the following formats:
- JSON (preferred)
- CSV

Files in raw JSON (one object per line) or compressed versions thereof (.zip, .gz or .tar.gz) are accepted. JSON objects should be in flat format. Nested JSON objects will not cause the upload to fail, but they will trigger a warning that nested fields will not be processed.

Below is an example of data in the acceptable JSON format:
```JSON
{"event_type":"registration", "user_id":"ABXD1232WER", "event_time":"1423160520015",
"time_zone":"UTC+0200", "IP":"113.245.239.117", "occurring_latitude":"16.99943733215332",
"occurring_longitude":"115.3338623046875", "device_client":"android",
"device_id":"bf23de564f7b7285613570a2e257d897", "device_version":"263"}
```

#### Data Sources
You have three options for importing the data files:
1. **Cloud**
    You can select a name for your dataset, and then select from a file(s) already uploaded to your assigned S3 bucket (or other cloud environment, e.g., HDFS/Azure/GCP/etc.) for deployment.

    The file path must follow such format: *data_directory/yyyyMMdd/raw_file*
    Data directory is the path of Select File From.
    E.g., *s3a://datavisor-dcubetest/uml-essential/134/data/20170508/rawlog_file.json*

   >Figure 5: Import Data From Cloud Step 1
   ![dcube-image-5]

   >Figure 6: Import Data From Cloud Step 2
   ![dcube-image-6]

2. **Upload Files**
   You can also upload file(s) using the web UI. For the uploaded file you can select the file type. If it is a .csv file, you can specify the operator, and if the first row is a header, then that row is ignored. The file size supported for upload is 100 MB.

   > Figure 7: Upload Data From Local
   ![dcube-image-7]

   Once you have selected or uploaded the data file, click **Next** to import the data.

   **Note:** During the import, the system will reveal any errors in the data file. There are a few errors that may block the process and require the error to be addressed before you can continue. These errors can include encoding issues, JSON formatting errors, or any unknown issues. Another example error may include “File too large.”

3. **MySQL**
   You can specify the database to choose from, write a query to pull the data, and preview the data before creating the dataset.
   > Figure 8: Import Data with MySQL
   ![dcube-image-8]

Once the data is imported, you have the option of transforming the imported data or directly proceeding to mapping.

### Import and Transform
> Figure 9: Data Management
![dcube-image-9]

Once the data is imported, you have multiple options to proceed. You may either view the data and feature distributions, transform the data in some way, or proceed to validate the dataset for use in modeling.

**Preview**
In this section, you can get a high-level view of the data (including the number of events, fields, and some metadata) and preview the data. A small subset of the data will be shown under the **Preview** section, in order to expedite easier understanding of the dataset.

> Figure 10: Preview Data
![dcube-image-10]

**Analyze**
Under the **Feature Distributions** tab, you can analyze the data using the histograms with sample data (30,000 records). The info from every field here will be displayed as a chart. You can switch between “%” (percentage) and “#”(number):

> Figure 11: Fields Analyze
![dcube-image-11]

**Transform**
In transformation step, you can do the following:
- Add, rename, or delete fields using the UI
- Create more powerful transformations using code (in Java)

***Add/Rename/Delete Fields***
Click the **Add Field** button to add fields and input the field name, field value, and data type (select String, Long, Date, Double). Multiple fields can be added in a single action.

***Concatenate Fields***
You can merge one or more fields to create a new field using a separator. You can merge only two fields at a time, but you can perform multiple merges in one step:
1. Select the first field and confirm whether you want to delete this field after merging.
2. Input the separator.
3. Select the second field and confirm whether you want to delete this field after merging.
4. Input the name of the new field.
> Figure 12: Merge Fields
![dcube-image-12]

***Filter Fields***
You can filter the dataset for modeling based on specific criteria, and events that satisfy these criteria will be stored as a new dataset (i.e. filter IN). Multiple conditions will be considered as an "AND" condition.
> Figure 13: Filter Fields
![dcube-image-13]

***Test Run***
Before creating this new dataset based on the above criteria, you have an option to run a test using "Test Run."
> Figure 14: Test Run
![dcube-image-14]

Once you are satisfied with the new dataset, you can click on **Save Changes as New Dataset** to create the new dataset.

**Join Dataset**
If you would like to join datasets together that were created based on database tables, you can specify the join criteria and the joining key.

### Import and Mapping
Prior to proceeding to data validation, the field formats for each field in the dataset must be finalized. This can be done by selecting a dataset and then clicking on **Finalize Fields**.

#### Match Features
An imported dataset that shows “Ready for Use” under **Status** can proceed to finalize its fields in order to continue with data validation.

In this step, you can set data types for each field in order for the fields to be correctly parsed in later steps. There are two actions that can be taken here:
1. Set a data type for a field in the dataset (String, Integer, Long, etc.)
2. Map a raw field to a known field name that was previously used in your dCube environment (a “Global Field”).

> Figure 15: Match Features
![dcube-image-15]

When you are finished with mapping raw fields to known fields, click the **VALIDATE FIELDS** button to proceed to data validation.

**Note:** Your initial mapping setting will be saved automatically after your first validation run. Should you go back to the **Field Mapping** page, your mapping setting will automatically be loaded for any new dataset. Any new changes (e.g. a new field) you make will update the setting. The UI will alert you if any required field for the scenario you chose is missing.

#### Data Validation
The data validation process provides a detailed description of your dataset and indicates whether there are any critical missing values.

If the data does not have any blocking issues, you can go to the next step, which is [Feature Engineering](#feature-engineering). However, if the data has any issues, you will need to fix them before moving onto the next step.

#### Data Validation Report
To investigate the issues, the data validation process will produce a **Validation Report** to provide a high-level summary of the data quality, alerts and warnings, as well as event and field distribution stats.
> Figure 16: Validation Report
![dcube-image-16]

***Summary***

This section shows overall stats about the data and its quality:
- \# of events processed
- \# and % of events successfully validated without problems
- \# and % of events that raised an alert
- \# and % of events that raised a warning

***Alerts, Warnings, and Notifications***

This section shows any errors or warnings at the object level. These can be classified into:
- Alerts: any deficiencies with the data that may result in unprocessed object(s). The missing entries are based on mandatory fields, but format alerts can apply for any field.
- Warnings: any possible deficiencies that may or may not need correction. This is based on recommended fields.
- Notifications: other items of note.

If more than 5% of the data has “Missing Values” alerts, you will not be able to proceed to the next step until you’ve successfully addressed those issues.
> Figure 17: Alerts, Warnings, and Notifications
![dcube-image-17]

The sample list of possible messages and trigger conditions are below:

**Invalid Format Alert (“invalid_format”):**
`Issue Found: The field “date” does not follow formatting guidelines in A% of all events.`
`Expected Format: “YYYY-MM-DD”`
`Actual Sample: “2/5/2017” (line 1655)`
This alert triggers when the format of the field value is not consistent with the format specified for the field during the parsing stage.

**Missing Value Warning (“missing_value”):**
`Potential Issue Found: The field “email” is empty or null in 31% of all events.`
This warning triggers when events contain a field, but the field value is empty or null for the recommended fields. There is no minimum threshold on occurrence (the message will trigger if a field has at least one empty or null value across all events).

**Invalid Value Warning (“invalid_value”):**
`Potential Issue Found: The field “ip” may have invalid values in 3% of all events.`<br>`Actual Sample: “255.255.255.0” (line 267)`
This warning triggers when a field has the correct formatting but has values known to be invalid or incorrect with high likelihood.

***Stats***

Different stats for mandatory fields are shown (e.g. event count, user count based on the data, IP count, and the number of users without a profile).

For each event type the number of records are displayed.
> Figure 18: Number of Records per Event Type
![dcube-image-18]

**Daily Event Count**
Based on the duration of the data, the distribution of the events as a time series is displayed.
> Figure 19: Daily Event Count
![dcube-image-19]

**Event and Field Distribution Stats**
This section shows the distribution of events and field values for further checks to see if the values are correct for each event. This includes:
- Number and percentage of total events per event type
- Distribution of timestamps in the source data (min/max timestamp and time series)
- Number of unique values for each field
- Number and percentage of the top 20 most common values out of total events for each field
> Figure 20: Event Count Distribution
![dcube-image-20]

> Figure 21: Event Count Table View
![dcube-image-21]

## Feature Management
dCube makes one of the most challenging components of machine learning fraud detection accessible by reducing the time of feature engineering.

During feature engineering, you can use out-of-the box features provided by dCube or create custom features using the **Create Feature** option and explore the distribution of each of the features.

From this step, you can proceed to build a model by using the **Create Model** option.

### Navigation
You can access **Feature Management** directly from the dCube **Home** page or from the **Data Management** page after data validation.

**Home -> Feature Platform**
There are two places where you can access **Feature Platform** through the dCube **Home** page:
1. Choose **Features -> Feature Engineering** from the top menu bar.
2. Directly access through the **Feature Platform** panel on the right side.  
> Figure 22: Feature Platform Navigation 1
![dcube-image-22]

**Data Management -> Dataset in Validation -> Proceed to Feature Engineering**
1. Choose the validated dataset that you want to move forward and click **Proceed to Feature Engineering**.

> Figure 23: Feature Platform Navigation 2.1
![dcube-image-23]

2. You can choose the out-of-box feature package in the following page, or click **Skip This Step** to access the Feature Management page.

> Figure 24: Feature Platform Navigation 2.2
![dcube-image-24]

Out-of-box features are organized based on specific use cases in feature packages. To access the features, users only need to choose a package and then map the required field name to the DV fields of the packages. [Section 3.3.2](#generate-out-of-box-features-through-feature-packages) will cover more details about the packages and how to generate out-of-box features.

### Feature Engineering
After running successful data validation, you can proceed to feature engineering. In the **Feature Management** page, there is a counter showing the number of raw features and custom features. Custom features are derived features created by users. Once created, these features stay in the system for direct use for the next dataset or users. To check existing custom features, click the **Customer Features** line, and the list of content will show.  

From this page, users can either create more derived features or proceed to **Create Model** if all features look good or. To create new customer features, clicking on **Create Feature** takes you to a separate page of **Feature Platform**, where users are able to create features using our out-of-box feature operator or by using code. Only published custom features in **Feature Platform** will be available in this **Feature Management** page as well as being used for the next modeling step.

> Figure 25: Feature Management Page
![dcube-image-25]

### Create Feature
#### Feature Platform Overview
**Feature Platform** is a standalone page and system to create new features while connecting with dCube. It can be reached from the top menu **Features -> Feature Platform** or through the **Create Feature** link in the **Feature Engineering** page.
> Figure 26: Feature Platform Navigation
![dcube-image-26]

Users have access to a variety of functions of **Feature Platform** through this home page. The first panel provides an overview of the number of dataset and features in the system. Users can review the details by clicking on the **VIEW** link for each item. Through this page, users can create new features or calculate all features in the dataset. The data uploading section connects back to the data studio.
> Figure 27: Feature Platform Page
![dcube-image-27]

**Feature Platform** also provides out-of-box features that are organized into various feature packages. Users can generate many features defined for a use case in a batch. For more advanced settings, users can create their own feature templates, functions, and packages.

In terms of workflow, we suggest users first explore our feature packages to check whether there are useful out-of-box features that can be used directly in their use case. After generating those out-of-box features, users can add additional features through the **Create Feature** button.  

#### Generate Out-of-Box Features through Feature Packages
You can add out-of-box features either after data validation or through the Feature Platform. It is located in the middle section of the **Feature Platform** home page, in **Start with a package** panel.
> Figure 28: Start with a Package
![dcube-image-28]

Currently there are 9 feature packages:
- Content abuse
- Transaction fraud
- Fake accounts
- General
- Application fraud
- Anti-money laundering
- ATO (account takeover)
- Promotion abuse
- GIN package

Except for the general and GIN packages, other packages are fraud case specific. Content abuse, fake accounts, and promotion abuse packages are mainly for social media use cases. Transaction fraud, application fraud, anti-money laundering, and ATO packages are for financial fraud. There is some overlap of features across different packages. Most of these features are generally used for a variety of fraud cases, such as IP and time related features. These common features are also grouped together in the general package, but users can select specific features for their use case.

***GIN Package*** (May require additional purchase)

To enhance detection efforts and enrich decision-making, DataVisor leverages its Global Intelligence Network (GIN), which comprises anonymized non-PII data from over 4 billion protected accounts and 800 billion events across the globe. The GIN contains rich information on digital data such as IP address subnets, prefixes, proxies and data centers, user agent strings, device types and OS, email address domains, and more. Information from the GIN feeds into machine learning algorithms to further improve overall detection.

A few examples of GIN features are:
GIN_DEVICE_TYPE_IP_COUNTRY_ALLUSER_COUNT
GIN_EMAIL_DOMAIN_IP_COUNTRY_SUSPICIOUS_USER_COUNT
GIN_EMAIL_DOMAIN_IP_COUNTRY_BADUSER_COUNT
GIN_PHONE_PREFIX_BADUSER_RATE
GIN_IP_FIRST_SEEN

GIN features can be used alone or to create new features. For example, if you would like to create a boolean feature that defines whether an IP subnet is likely to be fraudulent, you can create this new feature by encoding the GIN features “gin_ip20_baduser_rate” and “gin_ip20_alluser_count”. The former is the fraction of users accessing from an IP /20 subnet that is fraudulent, and the latter is the total users accessing from the IP subnet. Now you can create a boolean feature by using **Create by Coding** option ([Section 3.3.5](#create-by-coding)) to achieve the following logic:

When an IP subnet has more than 10 total users and more than 50% of these users are fraudulent, the boolean feature is set to “True”; otherwise, it is “False.” In this way, you can create a new feature that will be available for your modeling.

To check the list of packages and features for each package, click **Manage Packages**, then click the package name.

> Figure 29: Manage Packages
![dcube-image-29]

> Figure 30: Package List
![dcube-image-30]

> Figure 31: View Package
![dcube-image-31]

To get features from a package, click on the **Get More** button of the selected package to be navigated to the feature package page.
> Figure 32: Select a Package
![dcube-image-32]

> Figure 33: Get Features
![dcube-image-33]

Under each package, click on the field name. All the out-of-box features related to that field will be displayed.
> Figure 34: Field related Out-of-box Features
![dcube-image-34]

To use any of the features, select the field from your dataset that matches with the DataVisor Fields. For example, if the email field in the dataset is “email_address,” then you need to choose “email_address” to tag EMAIL. If the feature requires two or more fields, you need to make sure all the tags are matched to the raw data’s feature name. Feature dependency is shown at the end of each line. For example, because the AMOUNT related features require both USER_ID and AMOUNT fields, both the AMOUNT and USER_ID fields must be tagged to generate the AMOUNT related features.
> Figure 35: Feature Dependency example
![dcube-image-35]

> Figure 36: Feature Dependency example
![dcube-image-36]

However, it is not necessary to tag every field for each package. You just need to choose any features that are relevant to the use case. For example, if there is no PHONE field in the raw data, then there is no need to match that field or generate PHONE related features. After matching all necessary fields, click the **Get Features** button at the bottom, and all the related features in each field will be generated.   

> Figure 37: Generate Features
![dcube-image-37]

If there are some features that cannot be generated, an error message will appear on the page. Check the details by clicking on the **See More** button. If such a feature has already been created or if the user does not have a corresponding field for a feature they do not plan on using, then such an error is to be expected and that feature will not be created.
> Figure 38: Error Example 1
![dcube-image-38]

> Figure 39: Error Example 2
![dcube-image-39]

#### Create Regular Features
The **Feature Platform** provides the flexibility to create new features through either the UI or coding. Through UI, users can create features directly by using our out-of-box feature operator functions. Currently there are 4 different categories of functions that users can choose:
- Aggregation
- Attribute specific
- Region specific
- Generic
> Figure 40: Create New Features
![dcube-image-40]

Regular features that require minimal customization are those created from Generic, Attribute specific, and Region specific functions. Generic functions are simple Mathematical and String operations. Attribute specific functions are tailored to apply to certain attributes from the data (e.g. IP, email address, name). Region specific functions target country or region specific language or phone numbers. Each operator comes with a short description to explain how it works, and the following UI guides users on how to select parameters. The feature creation process for the three function categories are similar.  

***Example:*** Concatenate two strings
Once you choose a function, an explanation and examples will appear on the right side of the page.
> Figure 41: Function Explanation & Examples
![dcube-image-41]

Then the next section will guide users to choose parameters for this function and name the new feature.
> Figure 42: Parameters for Function & Feature Naming
![dcube-image-42]

After feature creation, the new features will show up in the feature list under the **Status** bar as “Draft.” Users can edit and test the draft features as needed.

> Figure 43: Feature Status
![dcube-image-43]

In the feature list page, users can check the feature information by clicking on the feature name. The pop up window provides a quick look on how the feature was defined. Users can choose to edit or test the feature through this page.
> Figure 44: Feature Information
![dcube-image-44]

Users can access more options by clicking on the “…” button at the end of each line, where users can also **Copy**, **See Dependency**, or **Delete** features.
> Figure 45: Other Options
![dcube-image-45]

The **See Dependency** function is useful to check how this feature is linked with other features.   
> Figure 46: Feature Dependency
![dcube-image-46]

However, if the feature is listed as “Published” under the **Status** bar, users cannot delete it, and they are only allowed to edit the description portion. We will cover the feature test and publish process later. A general suggestion is not to publish any feature until it has been thoroughly tested and confirmed.

#### Create Velocity Features
Velocity features are features based on Aggregation functions that can be used to do time series analysis. You can aggregate a specific attribute (data collection attribute) for each entity (aggregated by) under specific conditions. This is called an accumulator or aggregator. This accumulator can be reused for different functions such as count, distinct count, and more. You could also specify the time window for which you want to apply this function and any offset (start time).

***Example:*** Compute the total transaction amount of each client in the last 7 days.

**Function**: sum
**Data Collection Attribute**: amount
**Aggregated By**: client_id
**Condition**: event_type = transaction
**Window**: 7 days
**Offset**: 0

1. Choose the function “sum.”
> Figure 47: Sum Function
![dcube-image-47]
2. Define parameters in the aggregator. The system will generate a default Aggregator Name for each new aggregator, such as client_id_123. We recommend you change to a better name along with a brief description for easier lookup in the future.  Reusing an existing aggregator is preferred whenever possible, as it will make feature calculation faster. Conditions are optional in this section, but users leverage conditions to create more powerful features. Only events that match specified conditions will participate in feature calculation.
> Figure 48: Parameters for Aggregator
![dcube-image-48]
3. Define the time window. Choose the time scale on the right side of the bar, then select the start and end point of the time window. Users can also directly choose the time from the drop down menu under the bar. Select **Exclude Now** to exclude current time point from the time window.  
> Figure 49: Time Window
![dcube-image-49]

Users can also set time offset if the aggregation needs to be done for the time window in the past (e.g. from -30 days to -7 days). Below is a diagram explaining the concept of time window and offset.  
> Figure 50: Time Offset
![dcube-image-50]

4. Name the Feature and Save.
> Figure 51: Name and Save the Feature
![dcube-image-51]

***Create Velocity Feature Using Existing Aggregator***
If the aggregator exists, users can reuse it to create similar velocity features, which saves time and also makes the feature calculation much more efficient at the end. By changing functions, time window, or offset, a set of similar velocity features can be created.  

***Example***
By following the above example of the total transaction amount in the past 7 days, users can change the function to calculate the total transaction times for each client in the past 7 days. Start from **Create Feature**, choose **Count** function, and click **Existing Aggregator**. In the pop up window, choose the target aggregator (“amount_per_client in this case”) and click **Next**.
> Figure 52: Select Existing Aggregator
![dcube-image-52]

Then, the existing aggregator will be imported.  
 > Figure 53: Import Existing Aggregator
![dcube-image-53]

Continue to select the time window/offset and proceed to name the feature as previously described.

**Caveats**: Currently, **Feature Platform** does not support nesting of velocity features; the input feature of a velocity feature cannot be another velocity feature. This will be supported in the near future.

**Feature Platform** provides flexibility for users to aggregate data in different dimensions, thus creating features that are useful for downstream modeling. More examples of velocity features include:    
- Median or Stdev of transaction amount of each client in the past 3 month
- Number of transactions from an IP address in the last 24 hours
- Number of devices were used to login to the same account in the past 7 days
- Number of emails that linked to the same phone number in the past 180 days  

#### Create by Coding

dCube also supports creating features by coding. In the **Use Coding** tab, there will be a small editing window which currently supports Java, Python, and SQL.
 > Figure 54: Creating Feature by Code - Java
![dcube-image-54]

 > Figure 55: Creating Feature by Code - Python
![dcube-image-55]

 > Figure 56: Creating Feature by Code - SQL
![dcube-image-56]

The default page gives some example code for users to understand the format. To refer to a feature in the feature list, use “$featurename” to retrieve its value.   

***Example***: Transaction IP which is the IP only from a transaction event
 > Figure 57: Function Script - Java
![dcube-image-57]

 > Figure 58: Function Script - Python
![dcube-image-58]

#### Testing Features
Once a feature is created, users can backtest it by using a validated dataset. This step is important to check whether the new features work as expected. If any error occurs, users can edit the feature definition or adjust parameters. To test a feature, users can either click **Test** after naming the feature in the **Create New Feature** page or click the feature through the **Feature List** page.

In the **Test Feature** page, users need to first choose which dataset they want to use. Click the sample dataset name, and all validated dataset will show in the list. Choose a dataset that contains the proper data for testing purposes.  
 > Figure 59: Test Feature
![dcube-image-59]

After choosing the dataset, all available data files for this dataset will be shown on the page, and users need to choose one for testing (only one file at a time if there are multiple files). You can specify the number of records you want to use for testing from 500 to 10,000 records. Local mode is suggested for backtesting.
 > Figure 60: Specify the Number of Records
![dcube-image-60]

Click **Test** after setting up the above page. When the calculation is complete, the results table will show up in the lower half of the page. The feature in testing will be displayed in the first column, and the rest of the columns will be all the raw fields in the dataset. Users can select fewer columns through **Edit Columns** or sort values in specific columns for better viewability.
 > Figure 61: Results Table
![dcube-image-61]

#### Publish Features
After the new feature is well tested and confirmed, users can publish it through the feature list page. Features can be published individually by changing the status to “Published,” or users can batch select several features and click the **Publish** button on the top of the page. For features that depend on another feature, the root feature must be published first. All published features will be available in the **Feature Engineering** page.
 > Figure 62: Publish Features
![dcube-image-62]

#### Calculate Features (Optional)
After feature creation, testing, and publication, you may continue with dCube modeling with these features (refer to [Section 4.1](#feature-selection-and-subspace-formation)). Alternatively, if you would like to just apply these features to the entire dataset to obtain a new dataset, you can leverage the **Calculate Feature** on the **Feature Platform** home page. **Calculate** Feature processes all records in the selected dataset to calculate the values for derived features.
 > Figure 63: Calculate Features
![dcube-image-63]

Similarly to **Test Feature**, choose the dataset you want to use. All the validated datasets through Data Studio are listed in **Database**, and the user can also use the dataset in the Cloud.  
> Figure 64: Data Source
![dcube-image-64]

After choosing the dataset, select all files that need to be used for calculation and also select the Relay Mode. The **Feature Platform** supports three Replay Modes:
- Local -> run on the DataVisor single machine
- Distribute_Internal -> use DataVisor cluster
- Distribute_External -> launch external cluster
 > Figure 65: Replay Modes
![dcube-image-65]

After setting up the data source, choose which features that need to be calculated, name the task, and then click **Run**.  
 > Figure 66: Select Features to Compute
![dcube-image-66]

 > Figure 67: Click Run to Compute
![dcube-image-67]

Calculation progress and results can be viewed through **View Tasks**.
 > Figure 68: View Tasks
![dcube-image-68]

## dCube Manual Modeling
The following section will explore the fundamental components of the UML manual modeling with dCube.
### Feature Selection and Subspace Formation
Initially, no features are selected. Click the **ADD FEATURES** button shown below to add the features you want to include in the model.
 > Figure 69: Add Features Button
![dcube-image-69]

Next, the **Add Feature** window will pop up with all the available features that may be selected. You can search or filter the features by name or by type. You can also select or unselect all features within one or more types by enabling or disabling the option beside the type name.
 > Figure 70: Add Features Window
![dcube-image-70]

When your features have been selected, a **Smart Recommendation** window will appear at the top of the page. You may choose either to **Run Smart Recommendations** or **Skip This Step**  (details will be discussed in the next section).
 > Figure 71: Smart Recommendation Window
![dcube-image-71]

Meanwhile, the features that you have selected may be configured in several ways.

***Configuration***
There are three different types of features:
- **Selected Features (Normal)**: Feature selection defines the dimensions of the feature subspace. You can remove an existing feature from the selected by disabling the feature within the list.

   Use the **Add Features** button on the right to select features as additional ones.
   You can select or unselect a feature by toggling its switch on or off.

    > Figure 72: Select Features by Toggling the Switch
    ![dcube-image-72]

- **High Priority Features**: After either running or skipping the smart recommendation, a new column in the feature table named “Set as High Priority” will appear. These are must-have dimensions in the feature space. The high priority list can have at most 20 features. You can choose to enable or disable the high priority option in the table.

    > Figure 73: Set Feature Priority
    ![dcube-image-73]

- **Combined Features**: Combined features are high priority features by default  and are counted as one feature each in the maximum limit of 20 high priority features. You can select up to 3 features to generate 1 combined feature.

  1. Click on the three dots icon on the right to combine features.  

      > Figure 74: Click on the Three Dots Icon
      ![dcube-image-74]

  2. Select features to combine and click **SAVE**.

      > Figure 75: Select Features and Save
      ![dcube-image-75]

   3. Enable or disable a combined feature. When you disable a combined feature, the feature combination will be deleted and must be recreated for future use.

      > Figure 76: Disable Combination Feature
      ![dcube-image-76]

Remember to click the **Save** button beside the **ADD FEATURES** button when you leave the page, so that you can proceed with your current feature configurations next time. The same process applies for later configuration pages. As long as you click on **Save**, you may leave the current page and come back later.

To continue working on an in-progress **model configuration**, go to the Model Management home page, click on the three dots icon and select **View Model Config**. The system will bring you to the **Feature Selection** page with all your previously saved configurations loaded.

> Figure 77: View Model Config
![dcube-image-77]

### Running Smart Recommendations
This is an optional step in the manual model process. As suggested in the tooltips, running this job will perform following actions:
- Generate new dataset and feature distributions based on selected features
- Suggest high priority features based on feature importance
- Suggest correlated features and keywords to exclude (see details in the next section)

> Figure 78: Smart Recommendation
![dcube-image-78]

After you run smart recommendations, the system runs a job in the background to generate the recommendation information. Depending on the volume of data, this process may take 20 minutes or more.

> Figure 79: Running Smart Recommendation
![dcube-image-79]

You can wait for the job to finish or continue to configure features and proceed to the manual tuning step. When the job finishes, suggested high priority features will be tagged with yellow flags on the left side. You can click the **ACCEPT** button to enable all recommended features as high priority.

> Figure 80: Job Finished Page
![dcube-image-80]

The smart recommendation step also computes your feature distribution. You can also check the feature distribution by clicking the multi-colored bar icon to gain a better sense of the feature values and decide whether to include a feature in the model or tag it as high priority.

> Figure 81: Feature Distribution
![dcube-image-81]

After you have done all the configurations in the feature selection page, you can click the **Proceed to Manual Tuning** button in the top right corner to proceed to the next step.

### Clustering
#### Include or Exclude Feature Values
**Exclude Certain Values**
Smart recommendation provides recommended values of a feature that can be excluded from being used. The example below suggests to exclude an idfa value “00000000-0000-0000-0000-000000000000” as it may be a default value. By clicking on the value or clicking on **Add All**, this value will be excluded. You may add more values to exclude. The excluded values will not participate in linkage calculation and cluster scoring.

> Figure 82: Exclude Certain Values
![dcube-image-82]

***Configuration***
- Add recommendation: You can choose to **Add All** or click on specific values to add them to the configuration.
  > Figure 83: Add Recommendation
  ![dcube-image-83]
- Add a specific value: You can also manually type each value as part of the configuration.
  > Figure 84: Add a Specific Value
  ![dcube-image-84]

**Include Certain Values**
Similarly, you can use the include function to allow only specified values in clustering. One feature can be set to either exclude or include certain values, but not both. When viewed from the **INCLUDE KEYWORDS** tab, features with a warning sign indicate that the user has excluded those values.

> Figure 85: Include Keywords
![dcube-image-85]

**Exclude Global High Share Values**
An easier way to automatically exclude global common values is through configuring the “Global Share Threshold” column. Values with high global share are likely to be default values.
> Figure 86: Global Share Threshold
![dcube-image-86]

Feature values whose global shares are higher than this threshold will automatically be excluded. For example, suppose “10” is the most popular OS version with a global share of 0.12. By setting the global share threshold to 0.1, OS version value “10” will be excluded from modeling.

***Configuration***
Adjust the slider or enter a numerical value into the box to set the thresholds for each of the selected features. The value entered should be within the range [0, 1] inclusive.

> Figure 87: Set the Thresholds
![dcube-image-87]

#### Correlation
Based on the smart recommendation, default correlations are provided, and feature families are determined. The item in the blue box reflects the parent correlation that has precedence over the child correlation in the yellow box.

> Figure 88: Default Correlations
![dcube-image-88]

An example for a family feature is shown below.

> Figure 89: Family Feature
![dcube-image-89]

The address feature is the parent correlation of the child features: IP address, city, and cell area. The following child features all provide details about the location and are all relevant to the parent address feature. The cell area is the area code revealing the approximate region. The IP address reveals the location of the device. The city reveals the town of residence for a given address. In this context, the address is the parent feature as it provides the most granularity in regards to the objective of identifying the exact location of a user.

***Configuration***
- Click on **ADD ALL** to add the recommended feature family.
  > Figure 90: Add the Recommended Feature Family
  ![dcube-image-90]
- Modify the feature family by adding or deleting features.
  > Figure 91: Modify the Feature Family
  ![dcube-image-91]
- Create a new feature family.
  > Figure 92: Create a New Feature Family
  ![dcube-image-92]

#### Scoring
**Similarity Threshold**
All selected features are provided here to set the similarity threshold. By default, all values are set to 80%. A lower threshold value encourages a feature to participate more in cluster scoring. An extreme case: 100% means that only if all users inside a cluster share the same feature value will this feature be used for scoring.

***Configuration***
- Adjust the slider or enter a numerical value into the box. The value entered should be within the range [0, 1] inclusive.
  > Figure 93: Similarity Threshold
  ![dcube-image-93]

#### Filtering
**Basic Settings**
**Cluster Size:** The cluster size represents the minimum number of users per cluster. The system provides a default of 6.

The recommended cluster size is a minimum of 4-5 users per cluster. While a larger cluster size may detect larger groups, smaller groups may be overlooked,  resulting in more false negatives and compromising the recall. Based on the cluster size, you have an option to filter the clusters from the review. Note that this option is also available at the time of post-filtering to help you with the base model and create iterations on top of the base model by using post filtering.

The minimum number of selected features to match within a cluster helps measure the similarity of users within the group. The system provides a default of 1 feature.

***Configuration***
Enter a numerical value to set the cluster size and minimum number of selected features.
> Figure 94: Configuration for Filter Clusters
![dcube-image-94]

**Advanced Settings**
Users who are not similar to the rest of the cluster can be outliers, so filtering them out can usually boost model precision. We measure such similarity by the proportion of shared features a user has with the rest of the cluster. If such a portion is lower than the user outlier threshold, then this user becomes an outlier and will be filtered out. If the proportion of outliers in the cluster is greater than the cluster outlier threshold, the cluster will be filtered out.

***Configuration***
- Adjust the slider or enter a numerical value into the box. The value entered should be in the range [0, 1] inclusive.
> Figure 95: Advanced Settings
![dcube-image-95]

#### Save
At any point during the model step, you can save the configuration by clicking the **SAVE** button on the top right of the page.
> Figure 96: Save Button
![dcube-image-96]

#### Preview Configuration
After finishing all configurations, you can preview the settings before running the modeling by clicking on **Preview Configuration**.
> Figure 97: Preview Configuration
![dcube-image-97]

**Feature Configuration**
The **Feature Configuration** section includes information about high priority features and selected features, including the excluded or included values and the exclude share threshold.
> Figure 98: Feature Configuration
![dcube-image-98]

**Influencing Scores**
The **Influencing Scores** section displays information about the feature family and similarity scores.
> Figure 99: Influencing Scores
![dcube-image-99]

**Filtering Clusters**
The **Filtering Clusters** section displays information about the cluster filtering settings.
> Figure 100: Filtering Clusters
![dcube-image-100]

## Model Creation, Model Review & Model Iteration
Creating a model is the most important step in model building. Choosing a model suitable for the dataset and performing the corresponding model tuning procedure will help you achieve the optimal results.

### Model Selection
When your feature preparation is complete, select the corresponding dataset in the feature management interface and click the upper right corner **CREATE MODEL** button to start.
> Figure 101: Feature Management
![dcube-image-101]

You can create UML Auto Lite, UML Auto, UML Manual or Rule Engine models as needed. UML Auto model types are more suitable for social scenarios in which the system automatically configures everything and detects suspicious clusters. For different scenarios, we recommend using the UML Manual model type. The Rule Engine model is useful when you would like to create rules for detecting suspicious events. You can name the new model and add a brief description during the model review.
> Figure 102: Create Model
![dcube-image-102]

Upon selecting the model type, users must fill in three *mandatory* fields according to the dataset:
- Entity ID: Entity ID is a field that you would like to aggregate events by. In other words, it is the entity used to represent a node in a cluster.
- Event Type: Usually, a dataset may have different event types (e.g. registration, login, transaction, etc…). If your dataset consists of a single event type (thus lacking a field for data type), you may go back to [Feature Engineering](#feature-engineering) to create such feature via simple coding.
- Event Time: Timestamp of event.

Each field can be selected via the drop-down button. The **Label** function allows users to upload label data through the original data field. Labels can be used for model evaluation later on.

> Figure 103: Field Mapping for Model
![dcube-image-103]

Now, we will briefly introduce the four model types below:
- **UML Auto Lite:** UML Auto Lite will automatically build a lightweight version of the model based on DataVisor's intelligent recommendation features and configurations. The fraud distribution in the dataset helps facilitate subsequent fine-tuning.

  **Advantages:** Users can immediately click the **START** button after filling in the three mandatory fields. This model is a fully automatic system with a short runtime.

  > Figure 104: UML Auto Lite
  ![dcube-image-104]

- **UML Auto:** UML Auto adds feature selection functionality on top of the UML Auto Lite’s model. Once the required features have been manually added, unsupervised cluster analysis is performed to generate model results.

  **Advantages:** With semi-managed modeling, you have control over feature selection and feature weights. When weights are left empty, our system will compute them automatically.

  > Figure 105: UML Auto
  ![dcube-image-105]

- **UML Manual:** In contrast to the previous automatic models, UML Manual models can be fully configured by the user (e.g. feature selection, high priority feature selection, feature configurations such as including or excluding values, correlation between features, feature share threshold, minimum cluster size, etc...). When developing your first manual model, we suggest running the smart recommendation step to obtain and adopt the system’s recommended configurations. Users can then make more refined adjustments based on their own understanding of the use case and business.

- **Rule Engine:** Users can use the Rule Engine to create and run simple rules for detecting suspicious users and events. The purpose of the Rule Engine is to facilitate business-side rules to supplement and improve the overall model results.

### Model Configuration
Feature selection is applicable to both UML Auto and UML Manual. Users can click on the **ADD FEATURES** button to start the feature selection process.

> Figure 106: Add Features Button
![dcube-image-106]

In most cases, users can select all raw features by toggling the button in the “Raw Feature” row and then clicking the **Advanced Custom Feature** row to expand and select derived features to be included in a model. Note that a feature needs to be published before users can select it. If the feature is not yet published, clicking on the “Not Published” link will open a new feature platform page to publish that feature. Click on **ADD** to confirm your selection. After this step, users can further add or adjust features by clicking on the **ADD FEATURES** button.

> Figure 107: Select All Raw Features
![dcube-image-107]

> Figure 108: Advanced Custom Feature
![dcube-image-108]

You may start to run the model at this point for UML Auto. However, for UML Manual, the high priority features must be selected after overall feature selection is complete. You may run the smart recommendation step (refer to [Section 4.2](#running-smart-recommendations) for details) to get the system’s recommended high priority features, or skip this step for manual high priority feature configuration. A high priority feature serves as the fundamental element in a feature subspace. High priority features may be a single feature or a combined feature. Combine a feature with another feature (up to 3) by clicking on the three dots icon and then selecting **Combine feature with**.

> Figure 109: Select High Priority Features
![dcube-image-109]

After feature selection, you will enter a series of manual modeling configuration steps: **Refine Feature Values**, **Define Correlations**, **Influence Score**, and **Filter Clusters**. Click **PROCEED TO MANUAL TUNING** to complete each step. Please refer to [Section 4.3](#clustering) for details.

### Model Review
When the model’s status displays “Ready For Review” under the **Model Management** page, you can view the summary of the model results and begin the review process. Particularly for scenarios with a lack of labels, reviewing and labeling individual clusters will help estimate the model performance and provide new insights about tuning out false positives.

#### Summary of Model Results
By clicking **View Model Results** in the drop-down menu on the right hand side of the model, you can jump to the **Detection Summary** page. Here, you can get detection stats such as the number and percentage of detected users, overall precision, overall recall, and more.

> Figure 110: Detection Summary Page
![dcube-image-110]

The detection stats will dynamically change if the score threshold is increased from 0. For example, take 0.8 as shown below. A higher threshold usually results in improved precision and lower recall.

> Figure 111: Score Threshold
![dcube-image-111]

This page also visualizes the model performance via precision-score and recall-score graphs.

> Figure 112: Model Performance Visualization
![dcube-image-112]

#### Cluster Review
Under the **Model Management** page, click on the **Review and Label** button to go to the cluster review page.

> Figure 113: Review and Label Button
![dcube-image-113]

The **Cluster Review** page displays the total sampled clusters and several review stats. While these clusters are a subset of the overall detected clusters, they are sampled to represent each feature subspace for review purposes. Here, you can sort sample clusters by **Score**, **FP Rate** (false positive rate), **TP Rate** (true positive rate), **Similar Users** (number of detected users from the same subspace), and **Users** (cluster size). The **Reasons** column provides detailed detection reasons for a cluster. The full **Reasons** list can be opened by clicking on the small arrow on the leftmost side of the page. There is also a generic search bar that can search the cluster level content (e.g. cluster ID, reason string) and return matched records.

User can label a cluster as FP, TP, or Uncertain by clicking on the green, red or yellow button. To further view the users belonging to each cluster, click on the blue arrow to go to the **Cluster Detail** page (details in [Section 5.3.3](#cluster-review-filter)).

> Figure 114: Label a Cluster
![dcube-image-114]

To help you achieve higher review efficiency, we have developed the review filter functionality. By clicking on the **ADD FILTER** button, you may create nested filters to fetch the clusters you want to review the most. We will focus on this part in the next subsection. Above the cluster **FILTER**, there are three useful links:
- **MODEL CONFIGURATION**: Model configuration gives you quick access to the final configuration used to run the current model.
- **DATA DISTRIBUTION**: Data distribution is only available if the smart recommendation has been run.
- **COUNTERS**: Counters will provide important insights about various feature subspaces.

> Figure 115: Review Filter Functionality
![dcube-image-115]

Counters of both selected features and high priority features are provided in two charts. The higher the count, the more users are detected from the current feature. Take the feature “high_fraud_rate_client_id” below as an example. This feature helped detect 168 users and uniquely detected 87 users. However, if we remove this feature, we will miss 87 detections. Features with the highest counts (especially highest only counts) may be noisy and deserve more thorough checks. High priority features have similar counters. Each chart can also be viewed in percentage mode by toggling the # to % button on the top right of the window).

> Figure 116: Counters
![dcube-image-116]

#### Cluster Review Filter
To achieve higher review efficiency, you may use **FILTER CLUSTERS** to first review clusters of interest. Click on **ADD FILTER** and then configure your filter criteria.

Currently 6 types of filtering targets are supported:
- Selected features
- High priority features
- Similarity thresholds
- Cluster size
- Cluster score
- Number of selected features

After selecting the target you want to filter, enter the feature, operator and corresponding value you want to filter and click the + button to add the filter. Click on the **FILTER** button to apply all the filter criteria, and the cluster list below will be updated accordingly. You can keep adding filters to further restrict the returned clusters. In the example below, we create a filter for a selected feature (“high_fraud_rate_ip”) with the value true. After applying this filter, the total sampled clusters drops to 707 (from 2,717 before).

> Figure 117: Selected Features
![dcube-image-117]

> Figure 118: Applying Filter for Selected Feature
![dcube-image-118]

#### Cluster Detail Page
We can go to the **Cluster Details** page by clicking on the blue right arrow shown above. The **Cluster Details** page contains basic information about the cluster on the top. More importantly, it shows a detailed user table at the bottom that displays feature values for every detected user within the cluster. You can click the **Edit Columns** button to select features of interest or drag feature columns into a desired order. In addition to labeling the entire cluster, you can also label individual users with more granularity.

> Figure 119: Cluster Details
![dcube-image-119]

You can click on a user to visualize its similarity to other users in the cluster. The shared feature values will be highlighted in yellow.

> Figure 120: Visualize the Similarity for Certain User
![dcube-image-120]

Next to the **Cluster Details** tab is the **Similar Clusters** tab. This tab will show you other sampled clusters from the same feature subspace as the current cluster; the **Similar Users** count is the same for all clusters. Typically, clusters from the same feature subspace share similar performance because the detection reasons are highly similar. This provides you a quick way to label batches of similar clusters at the same time.

> Figure 121: Similar Clusters
![dcube-image-121]

#### Model Postfilter
If you want to postfilter your model to remove selective FP clusters, you can configure postfilter rules by clicking on the **RULES WORKSPACE** button and use these rules to re-run your model. Clusters and users that match ANY of your rule conditions will be filtered out.

> Figure 122: Postfilter Rules
![dcube-image-122]

Currently, we support three major rule types:
**Filter Clusters**: Within the Filter Clusters section, there are five subtypes.

> Figure 123: Filter Clusters
![dcube-image-123]

- **Cluster Detected Member Count** (cluster size): A cluster too small or too large may be a false positive.
- **Cluster Score**: A common rule is to filter out scores that are less than a user-defined threshold.
- **Cluster Feature Share**: This is an interesting rule that inspects the value share percentage of a selected feature. In the example below, clusters where more than 80% users share the same feature “last_name” will be filtered out.
  > Figure 124: Cluster Feature Share
  ![dcube-image-124]

- **Cluster Feature Count**: A common rule is to filter out clusters with a feature count that is less than a user-defined threshold. A lower feature count indicates a weaker feature subspace and detection reason.
- **FP Rate**: A common rule is to filter out clusters with high FP rates.

**Filter Users**: In the **Filter Users** section, you can create rules on the feature level to match users. The matched users will be excluded from the detection result later.

  > Figure 125: Filter Users
  ![dcube-image-125]

**Adjust Feature Score**: In the **Adjust Feature Score** section, there are two subtypes.
- **Feature Correlation**: You can define two correlated features that may not have been specified earlier during model configuration. The system will recompute cluster scores after taking this correlation into consideration. Sometimes, some clusters will be removed if the recomputed feature space is not not strong enough.
- **Feature Ignore**: You can define a feature that you want to be ignored entirely (in other words, removed from all feature subspaces). Usually, this is a stronger rule than the feature correlation rule.

  > Figure 126: Adjust Feature Score
  ![dcube-image-126]

After determining your postfilter rules, you can choose to either save directly or test the added rules. After clicking the **SAVE** button, the added rule will show up under the **Postfilter Rules** section. You can manage a set of created rules by using **CREATE RULESET**. If you need to test a rule, click the **TEST** button below to start the test. When the test is over, the results will be presented in the **TEST RESULTS** section. Here you can see the approximate number of filtered users and group information.

After consolidating your rules, you may click on **RE-RUN MODEL** which will trigger a postfilter job in the backend.

> Figure 127: Re-run Model
![dcube-image-127]

> Figure 128: Model Running
![dcube-image-128]

Now in the **Model Management** page, you can track the status of the post filtered model.

> Figure 129: Model Management Page
![dcube-image-129]

### Model Comparison
You can compare the detection results between any two models ready for review through the **Compare Model Results** interface. This interface enables users to quickly find iteration directions to improve model performance by comparing the models’ detection result differences under various thresholds.

You can access the **Model Comparison** page by clicking on the **Models** from the top menu bar.

> Figure 130: Model Comparison
![dcube-image-130]

After selecting the two models you want to compare, you can adjust the **Score Threshold for Model** to filter out the clusters with scores under your user-defined threshold.

> Figure 131: Score Threshold for Model
![dcube-image-131]

Then, click on **Compare**. The interface will display the total clusters, the number of unique clusters of the selected models, and the number of overlap clusters. You can also view user level differences.

> Figure 132: Model Comparison Results
![dcube-image-132]

### Model Iteration
To start from an existing model and make more adjustments, you can create a new model and use the import functionality. To do that, click on **Feature Engineering** from the Features top menu bar, choose the dataset you are working with, and click on **CREATE MODEL**.

> Figure 133: Create Model
![dcube-image-133]

Put in the input arguments required, then at the bottom of the model creation page, choose the model name you would like to copy from and click **PROCEED TO FEATURE SELECTION**. An alternative way of importing an existing model is to click on the **Import Configuration** within the **Feature Selection** page. However, keep in mind that the imported model will overwrite the current model configurations.

> Figure 134: Feature Selection
![dcube-image-134]

[dcube-image-1]: static/docs/images/en/dcube-image-1.png
[dcube-image-2]: static/docs/images/en/dcube-image-2.png
[dcube-image-3]: static/docs/images/en/dcube-image-3.png
[dcube-image-4]: static/docs/images/en/dcube-image-4.png
[dcube-image-5]: static/docs/images/en/dcube-image-5.png
[dcube-image-6]: static/docs/images/en/dcube-image-6.png
[dcube-image-7]: static/docs/images/en/dcube-image-7.png
[dcube-image-8]: static/docs/images/en/dcube-image-8.png
[dcube-image-9]: static/docs/images/en/dcube-image-9.png
[dcube-image-10]: static/docs/images/en/dcube-image-10.png
[dcube-image-11]: static/docs/images/en/dcube-image-11.png
[dcube-image-12]: static/docs/images/en/dcube-image-12.png
[dcube-image-13]: static/docs/images/en/dcube-image-13.png
[dcube-image-14]: static/docs/images/en/dcube-image-14.png
[dcube-image-15]: static/docs/images/en/dcube-image-15.png
[dcube-image-16]: static/docs/images/en/dcube-image-16.png
[dcube-image-17]: static/docs/images/en/dcube-image-17.png
[dcube-image-18]: static/docs/images/en/dcube-image-18.png
[dcube-image-19]: static/docs/images/en/dcube-image-19.png
[dcube-image-20]: static/docs/images/en/dcube-image-20.png
[dcube-image-21]: static/docs/images/en/dcube-image-21.png
[dcube-image-22]: static/docs/images/en/dcube-image-22.png
[dcube-image-23]: static/docs/images/en/dcube-image-23.png
[dcube-image-24]: static/docs/images/en/dcube-image-24.png
[dcube-image-25]: static/docs/images/en/dcube-image-25.png
[dcube-image-26]: static/docs/images/en/dcube-image-26.png
[dcube-image-27]: static/docs/images/en/dcube-image-27.png
[dcube-image-28]: static/docs/images/en/dcube-image-28.png
[dcube-image-29]: static/docs/images/en/dcube-image-29.png
[dcube-image-30]: static/docs/images/en/dcube-image-30.png
[dcube-image-31]: static/docs/images/en/dcube-image-31.png
[dcube-image-32]: static/docs/images/en/dcube-image-32.png
[dcube-image-33]: static/docs/images/en/dcube-image-33.png
[dcube-image-34]: static/docs/images/en/dcube-image-34.png
[dcube-image-35]: static/docs/images/en/dcube-image-35.png
[dcube-image-36]: static/docs/images/en/dcube-image-36.png
[dcube-image-37]: static/docs/images/en/dcube-image-37.png
[dcube-image-38]: static/docs/images/en/dcube-image-38.png
[dcube-image-39]: static/docs/images/en/dcube-image-39.png
[dcube-image-40]: static/docs/images/en/dcube-image-40.png
[dcube-image-41]: static/docs/images/en/dcube-image-41.png
[dcube-image-42]: static/docs/images/en/dcube-image-42.png
[dcube-image-43]: static/docs/images/en/dcube-image-43.png
[dcube-image-44]: static/docs/images/en/dcube-image-44.png
[dcube-image-45]: static/docs/images/en/dcube-image-45.png
[dcube-image-46]: static/docs/images/en/dcube-image-46.png
[dcube-image-47]: static/docs/images/en/dcube-image-47.png
[dcube-image-48]: static/docs/images/en/dcube-image-48.png
[dcube-image-49]: static/docs/images/en/dcube-image-49.png
[dcube-image-50]: static/docs/images/en/dcube-image-50.png
[dcube-image-51]: static/docs/images/en/dcube-image-51.png
[dcube-image-52]: static/docs/images/en/dcube-image-52.png
[dcube-image-53]: static/docs/images/en/dcube-image-53.png
[dcube-image-54]: static/docs/images/en/dcube-image-54.png
[dcube-image-55]: static/docs/images/en/dcube-image-55.png
[dcube-image-56]: static/docs/images/en/dcube-image-56.png
[dcube-image-57]: static/docs/images/en/dcube-image-57.png
[dcube-image-58]: static/docs/images/en/dcube-image-58.png
[dcube-image-59]: static/docs/images/en/dcube-image-59.png
[dcube-image-60]: static/docs/images/en/dcube-image-60.png
[dcube-image-61]: static/docs/images/en/dcube-image-61.png
[dcube-image-62]: static/docs/images/en/dcube-image-62.png
[dcube-image-63]: static/docs/images/en/dcube-image-63.png
[dcube-image-64]: static/docs/images/en/dcube-image-64.png
[dcube-image-65]: static/docs/images/en/dcube-image-65.png
[dcube-image-66]: static/docs/images/en/dcube-image-66.png
[dcube-image-67]: static/docs/images/en/dcube-image-67.png
[dcube-image-68]: static/docs/images/en/dcube-image-68.png
[dcube-image-69]: static/docs/images/en/dcube-image-69.png
[dcube-image-70]: static/docs/images/en/dcube-image-70.png
[dcube-image-71]: static/docs/images/en/dcube-image-71.png
[dcube-image-72]: static/docs/images/en/dcube-image-72.png
[dcube-image-73]: static/docs/images/en/dcube-image-73.png
[dcube-image-74]: static/docs/images/en/dcube-image-74.png
[dcube-image-75]: static/docs/images/en/dcube-image-75.png
[dcube-image-76]: static/docs/images/en/dcube-image-76.png
[dcube-image-77]: static/docs/images/en/dcube-image-77.png
[dcube-image-78]: static/docs/images/en/dcube-image-78.png
[dcube-image-79]: static/docs/images/en/dcube-image-79.png
[dcube-image-80]: static/docs/images/en/dcube-image-80.png
[dcube-image-81]: static/docs/images/en/dcube-image-81.png
[dcube-image-82]: static/docs/images/en/dcube-image-82.png
[dcube-image-83]: static/docs/images/en/dcube-image-83.png
[dcube-image-84]: static/docs/images/en/dcube-image-84.png
[dcube-image-85]: static/docs/images/en/dcube-image-85.png
[dcube-image-86]: static/docs/images/en/dcube-image-86.png
[dcube-image-87]: static/docs/images/en/dcube-image-87.png
[dcube-image-88]: static/docs/images/en/dcube-image-88.png
[dcube-image-89]: static/docs/images/en/dcube-image-89.png
[dcube-image-90]: static/docs/images/en/dcube-image-90.png
[dcube-image-91]: static/docs/images/en/dcube-image-91.png
[dcube-image-92]: static/docs/images/en/dcube-image-92.png
[dcube-image-93]: static/docs/images/en/dcube-image-93.png
[dcube-image-94]: static/docs/images/en/dcube-image-94.png
[dcube-image-95]: static/docs/images/en/dcube-image-95.png
[dcube-image-96]: static/docs/images/en/dcube-image-96.png
[dcube-image-97]: static/docs/images/en/dcube-image-97.png
[dcube-image-98]: static/docs/images/en/dcube-image-98.png
[dcube-image-99]: static/docs/images/en/dcube-image-99.png
[dcube-image-100]: static/docs/images/en/dcube-image-100.png
[dcube-image-101]: static/docs/images/en/dcube-image-101.png
[dcube-image-102]: static/docs/images/en/dcube-image-102.png
[dcube-image-103]: static/docs/images/en/dcube-image-103.png
[dcube-image-104]: static/docs/images/en/dcube-image-104.png
[dcube-image-105]: static/docs/images/en/dcube-image-105.png
[dcube-image-106]: static/docs/images/en/dcube-image-106.png
[dcube-image-107]: static/docs/images/en/dcube-image-107.png
[dcube-image-108]: static/docs/images/en/dcube-image-108.png
[dcube-image-109]: static/docs/images/en/dcube-image-109.png
[dcube-image-110]: static/docs/images/en/dcube-image-110.png
[dcube-image-111]: static/docs/images/en/dcube-image-111.png
[dcube-image-112]: static/docs/images/en/dcube-image-112.png
[dcube-image-113]: static/docs/images/en/dcube-image-113.png
[dcube-image-114]: static/docs/images/en/dcube-image-114.png
[dcube-image-115]: static/docs/images/en/dcube-image-115.png
[dcube-image-116]: static/docs/images/en/dcube-image-116.png
[dcube-image-117]: static/docs/images/en/dcube-image-117.png
[dcube-image-118]: static/docs/images/en/dcube-image-118.png
[dcube-image-119]: static/docs/images/en/dcube-image-119.png
[dcube-image-120]: static/docs/images/en/dcube-image-120.png
[dcube-image-121]: static/docs/images/en/dcube-image-121.png
[dcube-image-122]: static/docs/images/en/dcube-image-122.png
[dcube-image-123]: static/docs/images/en/dcube-image-123.png
[dcube-image-124]: static/docs/images/en/dcube-image-124.png
[dcube-image-125]: static/docs/images/en/dcube-image-125.png
[dcube-image-126]: static/docs/images/en/dcube-image-126.png
[dcube-image-127]: static/docs/images/en/dcube-image-127.png
[dcube-image-128]: static/docs/images/en/dcube-image-128.png
[dcube-image-129]: static/docs/images/en/dcube-image-129.png
[dcube-image-130]: static/docs/images/en/dcube-image-130.png
[dcube-image-131]: static/docs/images/en/dcube-image-131.png
[dcube-image-132]: static/docs/images/en/dcube-image-132.png
[dcube-image-133]: static/docs/images/en/dcube-image-133.png
[dcube-image-134]: static/docs/images/en/dcube-image-134.png




[image-1]: static/docs/images/en/1_RuleSet.png
[image-2]: static/docs/images/en/2_Workflow.png
[image-3]: static/docs/images/en/3_SelectFeature.png
[image-4]: static/docs/images/en/4_FeatureDetails.png
[image-5]: static/docs/images/en/5_SelectOperator.png
[image-6]: static/docs/images/en/6_OperatorSTR_EQ.png
[image-7]: static/docs/images/en/7_STR_IN_LIST.png
[image-8]: static/docs/images/en/8_RuleCondition.png
[image-9]: static/docs/images/en/9_UploadFile.png
[image-10]: static/docs/images/en/10_CopyCondition.png
[image-11]: static/docs/images/en/11_CountFunctionality.png
[image-12]: static/docs/images/en/12_ActionResponse.png
[image-13]: static/docs/images/en/13_CreateRuleSet.png
[image-14]: static/docs/images/en/14_AccessTestCenter.png
[image-15]: static/docs/images/en/15_BasicTest.png
[image-16]: static/docs/images/en/16_ToggleRuleRuleSet.png
[image-17]: static/docs/images/en/17_ManualInput.png
[image-18]: static/docs/images/en/18_BasicTestResults.png
[image-19]: static/docs/images/en/19_TestUserID.png
[image-20]: static/docs/images/en/20_Comparison.png
[image-21]: static/docs/images/en/21_ComparisonTestResults.png
[image-22]: static/docs/images/en/22_Simulation.png
[image-23]: static/docs/images/en/23_SimulationSelectRules.png
[image-24]: static/docs/images/en/24_SimulationTestResults.png
[image-25]: static/docs/images/en/25_AccessRuleDetails.png
[image-26]: static/docs/images/en/26_RuleDetails.png
[image-27]: static/docs/images/en/27_ToggleEventUser.png
